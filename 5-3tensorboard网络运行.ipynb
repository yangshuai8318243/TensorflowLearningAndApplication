{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0827 16:54:01.857117 20448 deprecation.py:323] From <ipython-input-1-0ac4527bf723>:8: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0827 16:54:01.863611 20448 deprecation.py:323] From E:\\TensorFlow\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0827 16:54:01.864630 20448 deprecation.py:323] From E:\\TensorFlow\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0827 16:54:02.102297 20448 deprecation.py:323] From E:\\TensorFlow\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0827 16:54:02.104286 20448 deprecation.py:323] From E:\\TensorFlow\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "W0827 16:54:02.150321 20448 deprecation.py:323] From E:\\TensorFlow\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Iter0,Testing Acc0.8256\n",
      "Iter1,Testing Acc0.8937\n",
      "Iter2,Testing Acc0.9013\n",
      "Iter3,Testing Acc0.9075\n",
      "Iter4,Testing Acc0.9091\n",
      "Iter5,Testing Acc0.9111\n",
      "Iter6,Testing Acc0.913\n",
      "Iter7,Testing Acc0.9138\n",
      "Iter8,Testing Acc0.9167\n",
      "Iter9,Testing Acc0.9166\n",
      "Iter10,Testing Acc0.9184\n",
      "Iter11,Testing Acc0.917\n",
      "Iter12,Testing Acc0.9188\n",
      "Iter13,Testing Acc0.9206\n",
      "Iter14,Testing Acc0.9198\n",
      "Iter15,Testing Acc0.9214\n",
      "Iter16,Testing Acc0.9211\n",
      "Iter17,Testing Acc0.9224\n",
      "Iter18,Testing Acc0.9219\n",
      "Iter19,Testing Acc0.9219\n",
      "Iter20,Testing Acc0.9219\n",
      "Iter21,Testing Acc0.9222\n",
      "Iter22,Testing Acc0.9231\n",
      "Iter23,Testing Acc0.9231\n",
      "Iter24,Testing Acc0.9235\n",
      "Iter25,Testing Acc0.9233\n",
      "Iter26,Testing Acc0.9229\n",
      "Iter27,Testing Acc0.9231\n",
      "Iter28,Testing Acc0.9244\n",
      "Iter29,Testing Acc0.9235\n",
      "Iter30,Testing Acc0.9245\n",
      "Iter31,Testing Acc0.9258\n",
      "Iter32,Testing Acc0.9243\n",
      "Iter33,Testing Acc0.9243\n",
      "Iter34,Testing Acc0.9249\n",
      "Iter35,Testing Acc0.9244\n",
      "Iter36,Testing Acc0.9258\n",
      "Iter37,Testing Acc0.9253\n",
      "Iter38,Testing Acc0.9265\n",
      "Iter39,Testing Acc0.9266\n",
      "Iter40,Testing Acc0.9263\n",
      "Iter41,Testing Acc0.9259\n",
      "Iter42,Testing Acc0.926\n",
      "Iter43,Testing Acc0.9269\n",
      "Iter44,Testing Acc0.9272\n",
      "Iter45,Testing Acc0.927\n",
      "Iter46,Testing Acc0.9276\n",
      "Iter47,Testing Acc0.9268\n",
      "Iter48,Testing Acc0.928\n",
      "Iter49,Testing Acc0.9287\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf;\n",
    "import matplotlib.pylab as plt;\n",
    "\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data \n",
    "\n",
    "# 载入数据\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\",one_hot = True)\n",
    "\n",
    "# 每个批次的大小\n",
    "batch_size = 100;\n",
    "# 计算一共有多少批次\n",
    "n_batch = mnist.train.num_examples // batch_size;\n",
    "\n",
    "# 参数概要\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope(\"summaries\"):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar(\"mean\",mean); #平均值\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))\n",
    "        tf.summary.scalar(\"stddev\",stddev) #标准差\n",
    "        tf.summary.scalar(\"max\",tf.reduce_max(var)) #最大值\n",
    "        tf.summary.scalar(\"min\",tf.reduce_min(var)) #最小值\n",
    "        tf.summary.histogram(\"histogram\",var)#直方图\n",
    "\n",
    "\n",
    "# 定义命名空间\n",
    "with tf.name_scope(\"input\"):\n",
    "    # 定义图片的站位数据\n",
    "    x = tf.placeholder(tf.float32,[None,784],name=\"x-input\")\n",
    "    y = tf.placeholder(tf.float32,[None,10],name=\"y-input\")\n",
    "\n",
    "with tf.name_scope(\"layer\"):\n",
    "    # 创建神经网络\n",
    "    with tf.name_scope(\"wights\"):\n",
    "        W = tf.Variable(tf.ones([784,10]),name=\"W\")\n",
    "        variable_summaries(W)\n",
    "    with tf.name_scope(\"biases\"):\n",
    "        b = tf.Variable(tf.ones([10]))\n",
    "        variable_summaries(b)\n",
    "    with tf.name_scope(\"wx_plus_b\"):\n",
    "        wx_plus_b = tf.matmul(x,W)+b\n",
    "    with tf.name_scope(\"softmax\"): \n",
    "        perdiction = tf.nn.softmax(wx_plus_b)\n",
    "\n",
    "# prediction = tf.tanh(Wx_plus_b_L2)\n",
    "# perdiction = tf.nn.softmax(Wx_plus_b_L2)\n",
    "\n",
    "\n",
    "# 定义二次函数 代价函数\n",
    "# loss = tf.reduce_mean(tf.square(y-perdiction))\n",
    "with tf.name_scope(\"loss\"): \n",
    "    # 交叉熵优化\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y,logits = perdiction))\n",
    "    tf.summary.scalar(\"loss\",loss)\n",
    "    \n",
    "    \n",
    "with tf.name_scope(\"train_setp\"): \n",
    "    # 使用梯度下降\n",
    "    train_setp = tf.train.GradientDescentOptimizer(0.22).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.name_scope(\"accuracy\"): \n",
    "    with tf.name_scope(\"correct_prediction\"): \n",
    "        # 将返回最大的值放在一个一维张量中\n",
    "        correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(perdiction,1))\n",
    "    with tf.name_scope(\"accuracy\"): \n",
    "        # 测试准确率\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "        tf.summary.scalar(\"accuracy\",accuracy)\n",
    "\n",
    "        \n",
    "# 合并所有检测的数据\n",
    "merged = tf.summary.merge_all();\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    writer = tf.summary.FileWriter(\"logs/\",sess.graph)\n",
    "    for epoch in range(50):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys = mnist.train.next_batch(batch_size);\n",
    "            run_options = tf.RunOptions(trace_level = tf.RunOptions.FULL_TRACE)\n",
    "            run_metadata = tf.RunMetadata()\n",
    "            summary,_ = sess.run([merged,train_setp],feed_dict={x:batch_xs,y:batch_ys},options=run_options, run_metadata=run_metadata)\n",
    "            \n",
    "        writer.add_summary(summary,epoch)\n",
    "        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "        print(\"Iter\" + str(epoch) + \",Testing Acc\"+ str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
